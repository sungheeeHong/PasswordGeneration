{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embedding Model\n",
    "## 본 과정에서 불러오는 Embedding 모델을 만드는 과정을 담고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset\n",
    "추후 추가될 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 및 사전 준비\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "pos_dict = {}\n",
    "\n",
    "# 여러 데이터셋에 대해서 tokenize, pos tag 진행 가능한 함수 생성. \n",
    "def tokenize_postag(txt, tokenizer, pos_dict):\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    pos_tokens = pos_tag(tokens)\n",
    "    for tag_tup in pos_tokens:\n",
    "        pos_dict.update({tag_tup[0]:tag_tup[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 불러오기\n",
    "import json\n",
    "with open(\"test-transcripts-aligned.json\") as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "# txt_dataset(list)에 json file의 utterance(내용 부분)만 꺼내서 담는다\n",
    "txt_dataset = []\n",
    "\n",
    "for i in data:\n",
    "    for j in data[i]:\n",
    "        txt_dataset.append(j['utterance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in txt_dataset:\n",
    "    tokenize_postag(txt, tokenizer=tokenizer, pos_dict=pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.book\n",
    "import nltk\n",
    "nltk.download(\"book\", quiet=True)\n",
    "from nltk.book import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset = []\n",
    "for i in range(1, 10):\n",
    "    text = globals()[f\"text{i}\"]\n",
    "    vocab = list(text.vocab().keys())\n",
    "    book_dataset.append(vocab) # ['Moby', 'Dick', '[', '}', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자만 있는 것 제거, lower()\n",
    "for book_num in range(len(book_dataset)):\n",
    "    book = book_dataset[book_num]\n",
    "    eraser_list = []\n",
    "    for vocab_num in range(len(book)):\n",
    "        vocab = book_dataset[book_num][vocab_num]\n",
    "        if not vocab.isalpha():\n",
    "            eraser_list.append(vocab)\n",
    "        elif vocab.isupper():\n",
    "            book_dataset[book_num][vocab_num] = vocab.lower()\n",
    "    for v in eraser_list:\n",
    "        book_dataset[book_num].remove(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag\n",
    "for book in book_dataset:\n",
    "    pos_tokens = pos_tag(book)\n",
    "    for tag_tup in pos_tokens:\n",
    "        pos_dict.update({tag_tup[0]:tag_tup[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.gutenberg\n",
    "corpus_name = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "corpus_list = []\n",
    "for name in corpus_name:\n",
    "    corpus_list.append(nltk.corpus.gutenberg.raw(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in corpus_list:\n",
    "    tokenize_postag(corpus, tokenizer=tokenizer, pos_dict=pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by POS tag, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VB_list = []\n",
    "JJ_list = []\n",
    "NN_list = []\n",
    "RT_list = []\n",
    "\n",
    "for key, value in pos_dict.items():\n",
    "    if \"VB\" in value:\n",
    "        VB_list.append(key)\n",
    "    elif \"JJ\" in value:\n",
    "        JJ_list.append(key)\n",
    "    elif \"NN\" in value:\n",
    "        NN_list.append(key)\n",
    "    else:\n",
    "        RT_list.append(key)\n",
    "    \n",
    "VB_list = np.array(VB_list)\n",
    "NN_list = np.array(NN_list)\n",
    "JJ_list = np.array(JJ_list)\n",
    "RT_list = np.array(RT_list)\n",
    "\n",
    "np.savez(\"pos_list\",\n",
    "            VB_list = VB_list,\n",
    "            JJ_list = JJ_list,\n",
    "            NN_list = NN_list,\n",
    "            RT_list = RT_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed word vector, save it\n",
    "품사별로 나눠서 embed하지 않고 한 번에 embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "vector_size = 128\n",
    "word_vector = FastText([list(pos_dict.keys())], vector_size = vector_size, window = 3, min_count = 1, workers = 1)\n",
    "word_vector.save(\"word_vector.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ptxt codebook, codebook embedding model, save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodebookModule import Codebook_HorizontalSliding\n",
    "PW_rule = Codebook_HorizontalSliding\n",
    "dic_codebook = {}\n",
    "\n",
    "for pos_name, pos_list in {'NN_list': NN_list, 'VB_list':VB_list, 'JJ_list':JJ_list, 'RT_list':RT_list}.items():\n",
    "    dict_pos = PW_rule(pos_list)\n",
    "    encoded_pos = [list(dict_pos.keys())]\n",
    "    encoded_pos_vec = FastText(encoded_pos, vector_size = vector_size, window = 3, min_count = 1, workers = 1)\n",
    "    try:\n",
    "        if pos_name == 'NN_list':\n",
    "            encoded_pos_vec.save(\"NN_pwd.model\")\n",
    "        elif pos_name == 'VB_list':\n",
    "            encoded_pos_vec.save(\"VB_pwd.model\")\n",
    "        elif pos_name == 'JJ_list':\n",
    "            encoded_pos_vec.save(\"JJ_pwd.model\")\n",
    "        else:\n",
    "            encoded_pos_vec.save(\"RT_pwd.model\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No such list or model: {pos_list}\")\n",
    "\n",
    "    pos_codebook = []\n",
    "    for pwd in encoded_pos_vec.wv.index_to_key:\n",
    "        temp = np.hstack((word_vector.wv[dict_pos[pwd]], encoded_pos_vec.wv[pwd]))\n",
    "        pos_codebook.append(temp)\n",
    "    dic_codebook.update({pos_name:pos_codebook})\n",
    "\n",
    "NN_codebook = dic_codebook['NN_list']\n",
    "VB_codebook = dic_codebook['VB_list']\n",
    "JJ_codebook = dic_codebook['JJ_list']\n",
    "RT_codebook = dic_codebook['RT_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"pos_codebook\",\n",
    "            VB_codebook = VB_codebook,\n",
    "            JJ_codebook = JJ_codebook,\n",
    "            NN_codebook = NN_codebook,\n",
    "            RT_codebook = RT_codebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
